apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    openshift.io/display-name: RedHatAI/Qwen2.5-7B-Instruct-FP8-dynamic
    serving.kserve.io/autoscalerClass: keda
    serving.kserve.io/deploymentMode: RawDeployment
    serving.kserve.io/stop: 'true'
  name: qwen25-7b-instruct
  namespace: autoscaling-demo
  labels:
    networking.kserve.io/visibility: exposed
    opendatahub.io/dashboard: 'true'
spec:
  predictor:
    autoScaling:
      metrics:
        - external:
            authenticationRef:
              authModes: bearer
              authenticationRef:
                name: inference-prometheus-auth
            metric:
              backend: prometheus
              query: 'vllm:num_requests_running'
              serverAddress: 'https://thanos-querier.openshift-monitoring.svc:9092'
            target:
              type: Value
              value: '1'
          type: External
    automountServiceAccountToken: false
    maxReplicas: 3
    minReplicas: 1
    model:
      args:
        - '--max-model-len'
        - '20000'
      modelFormat:
        name: vLLM
      name: ''
      resources:
        limits:
          cpu: '8'
          memory: 10Gi
          nvidia.com/gpu: '1'
        requests:
          cpu: '4'
          memory: 8Gi
          nvidia.com/gpu: '1'
      runtime: qwen25-7b-instruct
      storageUri: 'pvc://model-pvc-qwen3-8b/hf/instruction_tuned'
    tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Exists

